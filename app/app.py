# app.py

import streamlit as st
import pandas as pd
from datetime import date, timedelta

# ä»æˆ‘ä»¬çš„æ•°æ®å¤„ç†æ¨¡å—ä¸­å¯¼å…¥æ‰€æœ‰éœ€è¦çš„å‡½æ•°
import data_handler as dh

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="Aè‚¡å¹¶è´­äº‹ä»¶è¿½è¸ªå™¨",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

st.title('ğŸ“ˆ Aè‚¡å¹¶è´­äº‹ä»¶è¿½è¸ªå™¨ (ä¸“ä¸šç‰ˆ)')
st.markdown("æ•°æ®æ¥æº: å·¨æ½®èµ„è®¯ç½‘ | è´¢åŠ¡æ•°æ®: AkShare")

# --- 2. åº”ç”¨ç•Œé¢å¸ƒå±€ ---
with st.sidebar:
    st.header("ğŸ” ç­›é€‰æ¡ä»¶")
    today = date.today()
    default_start_date = today - timedelta(days=90)
    date_range = st.date_input(
        "é€‰æ‹©å…¬å‘Šæ—¥æœŸèŒƒå›´",
        value=(default_start_date, today),
        min_value=date(2010, 1, 1),
        max_value=today,
        format="YYYY-MM-DD"
    )
    st.info("æ¨èä½¿ç”¨ç²¾å‡†å…³é”®è¯ä»¥é¿å…æ— å…³å†…å®¹ã€‚")
    keywords_input = st.text_area(
        "è¾“å…¥æœç´¢å…³é”®è¯ (æ¯è¡Œä¸€ä¸ª)",
        "é‡å¤§èµ„äº§é‡ç»„é¢„æ¡ˆ\né‡å¤§èµ„äº§é‡ç»„è‰æ¡ˆ\nå‘è¡Œè‚¡ä»½è´­ä¹°èµ„äº§é¢„æ¡ˆ\nå‘è¡Œè‚¡ä»½è´­ä¹°èµ„äº§è‰æ¡ˆ"
    )
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]

# --- 3. ä¸»ç¨‹åºé€»è¾‘ ---
if st.sidebar.button('ğŸš€ å¼€å§‹æŠ“å–å’Œåˆ†æ'):
    if not keywords or len(date_range) != 2:
        st.error("è¯·è¾“å…¥å…³é”®è¯å¹¶é€‰æ‹©æœ‰æ•ˆçš„æ—¥æœŸèŒƒå›´ã€‚")
    else:
        start_date, end_date = date_range
        
        # å®ç°è‡ªåŠ¨æ•…éšœåˆ‡æ¢é€»è¾‘
        results_df = pd.DataFrame()
        try:
            with st.spinner('æ­£åœ¨ä»ä¸»æ•°æ®æº(å·¨æ½®èµ„è®¯ç½‘)æŠ“å–å…¬å‘Šåˆ—è¡¨...'):
                results_df = dh.scrape_cninfo(keywords, start_date, end_date)
            if results_df.empty:
                raise ValueError("ä¸»æ•°æ®æºæœªè¿”å›ä»»ä½•æ•°æ®ï¼Œå°è¯•å¤‡ç”¨æ•°æ®æºã€‚")
        except Exception:
            st.warning("âš ï¸ æ£€æµ‹åˆ°ä¸»æ•°æ®æº(å·¨æ½®)è®¿é—®å¤±è´¥æˆ–æ— ç»“æœï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®æº(ä¸œæ–¹è´¢å¯Œ)ã€‚")
            with st.spinner('æ­£åœ¨å°è¯•ä»å¤‡ç”¨æ•°æ®æºæŠ“å–...'):
                try:
                    results_df = dh.scrape_akshare(keywords, start_date, end_date)
                except Exception as e_ak:
                    st.error(f"å¤‡ç”¨æ•°æ®æº(AkShare)ä¹ŸæŠ“å–å¤±è´¥: {e_ak}")

        if results_df.empty:
            st.warning("åœ¨æŒ‡å®šæ¡ä»¶ä¸‹ï¼Œä¸»ã€å¤‡æ•°æ®æºå‡æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³å…¬å‘Šã€‚")
        else:
            st.success(f"æ‰¾åˆ° {len(results_df)} æ¡ç›¸å…³å…¬å‘Šï¼Œå¼€å§‹æ·±åº¦è§£æ...")
            
            with st.spinner('æ­£åœ¨æ‰¹é‡è·å–ä¸Šå¸‚å…¬å¸è´¢åŠ¡æ•°æ®...'):
                all_stock_codes = results_df['è‚¡ç¥¨ä»£ç '].unique().tolist()
                financial_data_df = dh.get_stock_financial_data(all_stock_codes)
            
            if not financial_data_df.empty:
                results_df = pd.merge(results_df, financial_data_df, on='è‚¡ç¥¨ä»£ç ', how='left')

            progress_bar = st.progress(0, text="æ­£åœ¨é€æ¡è§£æPDFå…¬å‘Šï¼Œè¯·è€å¿ƒç­‰å¾…...")
            extracted_data = []
            
            for index, row in results_df.iterrows():
                details = dh.extract_details_from_pdf(row['PDFé“¾æ¥'])
                row['æ‹Ÿå¹¶è´­å…¬å¸åç§°'], row['äº¤æ˜“å¯¹ä»·'], row['æ¶‰åŠäº¤æ˜“è‚¡ä¸œ'] = details
                extracted_data.append(row)
                progress_bar.progress((index + 1) / len(results_df), text=f"è§£æä¸­: {row['å…¬å¸åç§°']}")

            progress_bar.empty()
            final_df = pd.DataFrame(extracted_data)
            st.session_state['final_data'] = final_df

# --- 4. ç»“æœå±•ç¤º ---
if 'final_data' in st.session_state:
    st.success("æ•°æ®è§£æå®Œæˆï¼")
    final_df = st.session_state['final_data']
    st.markdown("---")
    st.subheader("ğŸ“Š åˆ†æç»“æœ")
    st.info("ç‚¹å‡»æ¯æ¡ç»“æœå‰çš„ `>` ç¬¦å·å¯å±•å¼€æŸ¥çœ‹ä¸Šå¸‚å…¬å¸è¯¦ç»†è´¢åŠ¡ä¿¡æ¯ã€‚")
    st.warning("AIè‡ªåŠ¨æå–çš„â€œæ‹Ÿå¹¶è´­å…¬å¸â€å’Œâ€œäº¤æ˜“å¯¹ä»·â€ç­‰ä¿¡æ¯å¯èƒ½ä¸å‡†ç¡®ï¼Œè¯·åŠ¡å¿…ç‚¹å‡»å…¬å‘Šé“¾æ¥è¿›è¡Œæ ¸å®ã€‚")

    for index, row in final_df.iterrows():
        summary_title = f"**{row['å…¬å¸åç§°']} ({row['è‚¡ç¥¨ä»£ç ']})** | {row['å…¬å‘Šæ—¥æœŸ']}"
        with st.expander(summary_title):
            st.markdown(f"**å…¬å‘Šæ ‡é¢˜**: {row['å…¬å‘Šæ ‡é¢˜']}")
            st.markdown(f"**å…¬å‘Šé“¾æ¥**: [ç‚¹å‡»æŸ¥çœ‹åŸæ–‡]({row['PDFé“¾æ¥']})")
            st.markdown("---")
            
            st.subheader("äº¤æ˜“æ ¸å¿ƒæ¦‚è¦ (AIæå–)")
            col1, col2, col3 = st.columns(3)
            col1.metric("æ‹Ÿå¹¶è´­å…¬å¸åç§°", row.get('æ‹Ÿå¹¶è´­å…¬å¸åç§°', 'N/A'))
            col2.metric("äº¤æ˜“å¯¹ä»·", row.get('äº¤æ˜“å¯¹ä»·', 'N/A'))
            col3.text_area("æ¶‰åŠäº¤æ˜“è‚¡ä¸œ", row.get('æ¶‰åŠäº¤æ˜“è‚¡ä¸œ', 'N/A'), height=100, disabled=True)
            
            st.markdown("---")

            st.subheader("ä¸Šå¸‚å…¬å¸å¿«ç…§")
            col_a, col_b, col_c = st.columns(3)
            col_a.metric("æ€»å¸‚å€¼ (äº¿å…ƒ)", f"{row.get('æ€»å¸‚å€¼', 0) / 1e8:.2f}" if pd.notna(row.get('æ€»å¸‚å€¼')) else "N/A")
            col_b.metric("å¸‚ç›ˆç‡ (åŠ¨æ€)", f"{row.get('å¸‚ç›ˆç‡-åŠ¨æ€'):.2f}" if pd.notna(row.get('å¸‚ç›ˆç‡-åŠ¨æ€')) else "N/A")
            col_c.metric("èµ„äº§è´Ÿå€ºç‡ (%)", f"{row.get('èµ„äº§è´Ÿå€ºç‡(%)'):.2f}" if pd.notna(row.get('èµ„äº§è´Ÿå€ºç‡(%)')) else "N/A")
            
            st.text_area("è¡Œä¸šé¢˜æ", row.get('è¡Œä¸šé¢˜æ', 'N/A'), height=100, disabled=True)
