# app.py (v2.1 - å·²ä¿®æ­£é”™è¯¯å¹¶æ•´åˆå¤‡ç”¨æ•°æ®æºé€»è¾‘)
import streamlit as st
import requests
import pandas as pd
import akshare as ak
from datetime import datetime, date, timedelta
import time
import re
from io import BytesIO
from PyPDF2 import PdfReader

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="Aè‚¡å¹¶è´­äº‹ä»¶è¿½è¸ªå™¨",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

st.title('ğŸ“ˆ Aè‚¡å¹¶è´­äº‹ä»¶è¿½è¸ªå™¨ (ä¸“ä¸šç‰ˆ)')
st.markdown("æ•°æ®æ¥æº: å·¨æ½®èµ„è®¯ç½‘ | è´¢åŠ¡æ•°æ®: AkShare")

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

@st.cache_data(ttl=3600)
def get_stock_financial_data(stock_codes):
    """è·å–ä¸Šå¸‚å…¬å¸çš„è¯¦ç»†è´¢åŠ¡å’Œè¡Œæƒ…æ•°æ®"""
    # (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œä¿æŒåŸæ ·)
    data_df = pd.DataFrame()
    if not stock_codes:
        return data_df
        
    try:
        quote_df = ak.stock_zh_a_spot_em()
        quote_df = quote_df[quote_df['ä»£ç '].isin(stock_codes)]
        quote_df = quote_df[['ä»£ç ', 'æ€»å¸‚å€¼', 'å¸‚ç›ˆç‡-åŠ¨æ€']]
        quote_df.rename(columns={'ä»£ç ': 'è‚¡ç¥¨ä»£ç '}, inplace=True)
        data_df = quote_df
    except Exception as e:
        st.warning(f"è·å–è¡Œæƒ…æ•°æ®æ—¶å‡ºé”™: {e}")

    try:
        temp_financials = []
        for code in stock_codes:
            try:
                df = ak.stock_financial_analysis_indicator(stock_code=code)
                latest_financials = df.iloc[0][['èµ„äº§è´Ÿå€ºç‡(%)']]
                latest_financials['è‚¡ç¥¨ä»£ç '] = code
                temp_financials.append(latest_financials)
            except:
                continue
        if temp_financials:
            financial_df = pd.DataFrame(temp_financials)
            if not data_df.empty:
                data_df = pd.merge(data_df, financial_df, on='è‚¡ç¥¨ä»£ç ', how='left')
            else:
                data_df = financial_df
    except Exception as e:
        st.warning(f"è·å–è´¢åŠ¡æŒ‡æ ‡æ—¶å‡ºé”™: {e}")

    try:
        industry_df = ak.stock_board_concept_name_em()
        temp_concepts = []
        for code in stock_codes:
            try:
                concepts = industry_df[industry_df['ä»£ç '] == code]['æ¦‚å¿µåç§°'].str.cat(sep=', ')
                temp_concepts.append({'è‚¡ç¥¨ä»£ç ': code, 'è¡Œä¸šé¢˜æ': concepts})
            except:
                continue
        if temp_concepts:
            concept_df = pd.DataFrame(temp_concepts)
            if not data_df.empty:
                data_df = pd.merge(data_df, concept_df, on='è‚¡ç¥¨ä»£ç ', how='left')
            else:
                data_df = concept_df
    except Exception as e:
        st.warning(f"è·å–è¡Œä¸šé¢˜ææ—¶å‡ºé”™: {e}")
        
    return data_df


def extract_details_from_pdf(pdf_url):
    """(AIå°½åŠ›è€Œä¸º) å°è¯•ä»PDFå…¬å‘Šä¸­æå–å…³é”®ä¿¡æ¯ã€‚"""
    # (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œä¿æŒåŸæ ·)
    target_company = "æœªæå–åˆ°"
    transaction_price = "æœªæå–åˆ°"
    shareholders = "æœªæå–åˆ°"
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(pdf_url, headers=headers, timeout=20)
        response.raise_for_status()
        
        with BytesIO(response.content) as pdf_file:
            reader = PdfReader(pdf_file)
            text = ""
            num_pages_to_read = min(len(reader.pages), 10)
            for i in range(num_pages_to_read):
                page = reader.pages[i]
                if page.extract_text():
                    text += page.extract_text()
        
        text = text.replace("\n", "").replace(" ", "")

        match_target = re.search(r'(?:æ ‡çš„å…¬å¸|æ ‡çš„èµ„äº§|äº¤æ˜“æ ‡çš„)ä¸º(\w+?å…¬å¸)', text)
        if match_target:
            target_company = match_target.group(1)
        else:
            match_target = re.search(r'æ‹Ÿè´­ä¹°(\w+?å…¬å¸)', text)
            if match_target:
                target_company = match_target.group(1)

        match_price = re.search(r'(?:äº¤æ˜“ä½œä»·|äº¤æ˜“ä»·æ ¼|äº¤æ˜“å¯¹ä»·)(?:æš‚å®šä¸º|ä¸º|åˆè®¡ä¸º)([\d,.\s]+)(?:å…ƒ|ä¸‡å…ƒ|äº¿å…ƒ)', text)
        if match_price:
            price_value = match_price.group(1)
            price_unit = match_price.group(2) if len(match_price.groups()) > 1 and match_price.group(2) else "å…ƒ"
            transaction_price = f"{price_value}{price_unit}"

        match_shareholders = re.search(r'äº¤æ˜“å¯¹æ–¹ä¸º([\wã€ï¼Œ,ï¼ˆï¼‰()]+?)[ï¼Œã€‚]', text)
        if match_shareholders:
            shareholders = match_shareholders.group(1).strip('ï¼Œ').strip('ã€‚')
    except Exception:
        pass
    return target_company, transaction_price, shareholders


@st.cache_data(ttl=3600)
def scrape_cninfo(keywords, start_date, end_date):
    """ä¸»æ•°æ®æºï¼šæ ¹æ®å…³é”®è¯å’Œæ—¥æœŸèŒƒå›´ï¼Œä»å·¨æ½®èµ„è®¯ç½‘æŠ“å–å…¬å‘Šåˆ—è¡¨"""
    # (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œä¿æŒåŸæ ·)
    api_url = "http://www.cninfo.com.cn/new/hisAnnouncement/query"
    headers = {'User-Agent': 'Mozilla/5.0'}
    str_start_date = start_date.strftime('%Y-%m-%d')
    str_end_date = end_date.strftime('%Y-%m-%d')
    all_announcements = []
    for keyword in keywords:
        page_num = 1
        while True:
            params = {
                'pageNum': page_num, 'pageSize': 30, 'column': 'sse,szse,hk',
                'tabName': 'fulltext', 'searchkey': keyword,
                'seDate': f'{str_start_date}~{str_end_date}', 'isHLtitle': 'true'
            }
            try:
                response = requests.post(api_url, headers=headers, data=params, timeout=20)
                response.raise_for_status()
                data = response.json()
            except Exception as e:
                st.error(f"è¯·æ±‚å·¨æ½®èµ„è®¯ç½‘å¤±è´¥: {e}ï¼Œè¯·ç¨åå†è¯•ã€‚")
                raise e # æŠ›å‡ºå¼‚å¸¸ä»¥è§¦å‘å¤‡ç”¨æ–¹æ¡ˆ
            
            announcements = data.get('announcements', [])
            if not announcements:
                break
            all_announcements.extend(announcements)
            if len(announcements) < 30:
                break
            page_num += 1
            time.sleep(0.3)
    if not all_announcements:
        return pd.DataFrame()
    df = pd.DataFrame(all_announcements)
    df = df[['secCode', 'secName', 'announcementTitle', 'announcementTime', 'adjunctUrl']]
    df.rename(columns={
        'secCode': 'è‚¡ç¥¨ä»£ç ', 'secName': 'å…¬å¸åç§°', 'announcementTitle': 'å…¬å‘Šæ ‡é¢˜',
        'announcementTime': 'å…¬å‘Šæ—¥æœŸ', 'adjunctUrl': 'PDFè·¯å¾„'
    }, inplace=True)
    df['å…¬å‘Šæ—¥æœŸ'] = pd.to_datetime(df['å…¬å‘Šæ—¥æœŸ'], unit='ms').dt.strftime('%Y-%m-%d')
    df['PDFé“¾æ¥'] = 'http://static.cninfo.com.cn/' + df['PDFè·¯å¾„']
    df.drop_duplicates(subset=['è‚¡ç¥¨ä»£ç ', 'å…¬å‘Šæ ‡é¢˜', 'å…¬å‘Šæ—¥æœŸ'], inplace=True)
    df.sort_values(by='å…¬å‘Šæ—¥æœŸ', ascending=False, inplace=True)
    return df.drop(columns=['PDFè·¯å¾„'])

# ã€å…³é”®ä¿®æ­£1ã€‘å°† scrape_akshare å‡½æ•°å®šä¹‰ç§»åˆ°è¿™é‡Œ
@st.cache_data(ttl=3600)
def scrape_akshare(keywords, start_date, end_date):
    """å¤‡ç”¨æ•°æ®æºï¼šä½¿ç”¨ AkShare ä»ä¸œæ–¹è´¢å¯Œç½‘æŠ“å–å…¬å‘Šã€‚"""
    st.warning("âš ï¸ æ£€æµ‹åˆ°ä¸»æ•°æ®æº(å·¨æ½®)è®¿é—®å¤±è´¥æˆ–æ— ç»“æœï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®æº(ä¸œæ–¹è´¢å¯Œ)ã€‚")
    str_start_date = start_date.strftime('%Y%m%d')
    str_end_date = end_date.strftime('%Y%m%d')
    try:
        all_notices_df = ak.stock_notice_report(start_date=str_start_date, end_date=str_end_date)
        if all_notices_df.empty:
            return pd.DataFrame()
        keyword_pattern = '|'.join(keywords)
        filtered_df = all_notices_df[all_notices_df['å…¬å‘Šæ ‡é¢˜'].str.contains(keyword_pattern, na=False)].copy()
        if filtered_df.empty:
            return pd.DataFrame()
        filtered_df.rename(columns={
            'è‚¡ç¥¨ä»£ç ': 'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨ç®€ç§°': 'å…¬å¸åç§°', 'å…¬å‘Šæ ‡é¢˜': 'å…¬å‘Šæ ‡é¢˜',
            'å…¬å‘Šæ—¥æœŸ': 'å…¬å‘Šæ—¥æœŸ', 'å…¬å‘Šé“¾æ¥': 'PDFé“¾æ¥'
        }, inplace=True)
        filtered_df['å…¬å‘Šæ—¥æœŸ'] = pd.to_datetime(filtered_df['å…¬å‘Šæ—¥æœŸ']).dt.strftime('%Y-%m-%d')
        final_df = filtered_df[['è‚¡ç¥¨ä»£ç ', 'å…¬å¸åç§°', 'å…¬å‘Šæ ‡é¢˜', 'å…¬å‘Šæ—¥æœŸ', 'PDFé“¾æ¥']]
        final_df.sort_values(by='å…¬å‘Šæ—¥æœŸ', ascending=False, inplace=True)
        return final_df
    except Exception as e:
        st.error(f"å¤‡ç”¨æ•°æ®æº(AkShare)ä¹ŸæŠ“å–å¤±è´¥: {e}")
        return pd.DataFrame()

# --- åº”ç”¨ç•Œé¢å¸ƒå±€ ---
with st.sidebar:
    st.header("ğŸ” ç­›é€‰æ¡ä»¶")
    today = date.today()
    default_start_date = today - timedelta(days=90)
    date_range = st.date_input(
        "é€‰æ‹©å…¬å‘Šæ—¥æœŸèŒƒå›´",
        value=(default_start_date, today),
        min_value=date(2010, 1, 1),
        max_value=today,
        format="YYYY-MM-DD"
    )
    st.info("æ¨èä½¿ç”¨ç²¾å‡†å…³é”®è¯ä»¥é¿å…æ— å…³å†…å®¹ã€‚")
    keywords_input = st.text_area(
        "è¾“å…¥æœç´¢å…³é”®è¯ (æ¯è¡Œä¸€ä¸ª)",
        "é‡å¤§èµ„äº§é‡ç»„é¢„æ¡ˆ\né‡å¤§èµ„äº§é‡ç»„è‰æ¡ˆ\nå‘è¡Œè‚¡ä»½è´­ä¹°èµ„äº§é¢„æ¡ˆ\nå‘è¡Œè‚¡ä»½è´­ä¹°èµ„äº§è‰æ¡ˆ"
    )
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]

if st.sidebar.button('ğŸš€ å¼€å§‹æŠ“å–å’Œåˆ†æ'):
    if not keywords or len(date_range) != 2:
        st.error("è¯·è¾“å…¥å…³é”®è¯å¹¶é€‰æ‹©æœ‰æ•ˆçš„æ—¥æœŸèŒƒå›´ã€‚")
    else:
        start_date, end_date = date_range
        
        # ã€å…³é”®ä¿®æ­£2ã€‘åœ¨è¿™é‡Œå®ç°è‡ªåŠ¨æ•…éšœåˆ‡æ¢é€»è¾‘
        results_df = pd.DataFrame()
        try:
            with st.spinner('æ­£åœ¨ä»ä¸»æ•°æ®æº(å·¨æ½®èµ„è®¯ç½‘)æŠ“å–å…¬å‘Šåˆ—è¡¨...'):
                results_df = scrape_cninfo(keywords, start_date, end_date)
            if results_df.empty:
                raise ValueError("ä¸»æ•°æ®æºæœªè¿”å›ä»»ä½•æ•°æ®ï¼Œå°è¯•å¤‡ç”¨æ•°æ®æºã€‚")
        except Exception as e:
            print(f"ä¸»æ•°æ®æºå¤±è´¥: {e}") # åœ¨åå°æ‰“å°é”™è¯¯æ—¥å¿—
            with st.spinner('ä¸»æ•°æ®æºå¼‚å¸¸ï¼Œæ­£åœ¨å°è¯•ä»å¤‡ç”¨æ•°æ®æºæŠ“å–...'):
                results_df = scrape_akshare(keywords, start_date, end_date)
        
        if results_df.empty:
            st.warning("åœ¨æŒ‡å®šæ¡ä»¶ä¸‹ï¼Œä¸»ã€å¤‡æ•°æ®æºå‡æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³å…¬å‘Šã€‚")
        else:
            st.success(f"æ‰¾åˆ° {len(results_df)} æ¡ç›¸å…³å…¬å‘Šï¼Œå¼€å§‹æ·±åº¦è§£æ...")
            
            all_stock_codes = results_df['è‚¡ç¥¨ä»£ç '].unique().tolist()
            with st.spinner('æ­£åœ¨æ‰¹é‡è·å–ä¸Šå¸‚å…¬å¸è´¢åŠ¡æ•°æ®...'):
                financial_data_df = get_stock_financial_data(all_stock_codes)
            
            if not financial_data_df.empty:
                results_df = pd.merge(results_df, financial_data_df, on='è‚¡ç¥¨ä»£ç ', how='left')

            progress_bar = st.progress(0, text="æ­£åœ¨é€æ¡è§£æPDFå…¬å‘Šï¼Œè¯·è€å¿ƒç­‰å¾…...")
            extracted_data = []
            
            for index, row in results_df.iterrows():
                target_company, price, shareholders = extract_details_from_pdf(row['PDFé“¾æ¥'])
                row['æ‹Ÿå¹¶è´­å…¬å¸åç§°'] = target_company
                row['äº¤æ˜“å¯¹ä»·'] = price
                row['æ¶‰åŠäº¤æ˜“è‚¡ä¸œ'] = shareholders
                extracted_data.append(row)
                progress_bar.progress((index + 1) / len(results_df), text=f"è§£æä¸­: {row['å…¬å¸åç§°']}")

            progress_bar.empty()
            final_df = pd.DataFrame(extracted_data)
            st.session_state['final_data'] = final_df

if 'final_data' in st.session_state:
    st.success("æ•°æ®è§£æå®Œæˆï¼")
    final_df = st.session_state['final_data']
    st.markdown("---")
    st.subheader("ğŸ“Š åˆ†æç»“æœ")
    st.info("ç‚¹å‡»æ¯æ¡ç»“æœå‰çš„ `>` ç¬¦å·å¯å±•å¼€æŸ¥çœ‹ä¸Šå¸‚å…¬å¸è¯¦ç»†è´¢åŠ¡ä¿¡æ¯ã€‚")
    st.warning("AIè‡ªåŠ¨æå–çš„â€œæ‹Ÿå¹¶è´­å…¬å¸â€å’Œâ€œäº¤æ˜“å¯¹ä»·â€ç­‰ä¿¡æ¯å¯èƒ½ä¸å‡†ç¡®ï¼Œè¯·åŠ¡å¿…ç‚¹å‡»å…¬å‘Šé“¾æ¥è¿›è¡Œæ ¸å®ã€‚")

    for index, row in final_df.iterrows():
        summary_title = f"**{row['å…¬å¸åç§°']} ({row['è‚¡ç¥¨ä»£ç ']})** | {row['å…¬å‘Šæ—¥æœŸ']}"
        with st.expander(summary_title):
            st.markdown(f"**å…¬å‘Šæ ‡é¢˜**: {row['å…¬å‘Šæ ‡é¢˜']}")
            st.markdown(f"**å…¬å‘Šé“¾æ¥**: [ç‚¹å‡»æŸ¥çœ‹åŸæ–‡]({row['PDFé“¾æ¥']})")
            st.markdown("---")
            
            st.subheader("äº¤æ˜“æ ¸å¿ƒæ¦‚è¦ (AIæå–)")
            col1, col2, col3 = st.columns(3)
            col1.metric("æ‹Ÿå¹¶è´­å…¬å¸åç§°", row.get('æ‹Ÿå¹¶è´­å…¬å¸åç§°', 'N/A'))
            col2.metric("äº¤æ˜“å¯¹ä»·", row.get('äº¤æ˜“å¯¹ä»·', 'N/A'))
            col3.text_area("æ¶‰åŠäº¤æ˜“è‚¡ä¸œ", row.get('æ¶‰åŠäº¤æ˜“è‚¡ä¸œ', 'N/A'), height=100, disabled=True)
            
            st.markdown("---")

            st.subheader("ä¸Šå¸‚å…¬å¸å¿«ç…§")
            col_a, col_b, col_c = st.columns(3)
            col_a.metric("æ€»å¸‚å€¼ (äº¿å…ƒ)", f"{row.get('æ€»å¸‚å€¼', 0) / 1e8:.2f}" if pd.notna(row.get('æ€»å¸‚å€¼')) else "N/A")
            col_b.metric("å¸‚ç›ˆç‡ (åŠ¨æ€)", f"{row.get('å¸‚ç›ˆç‡-åŠ¨æ€'):.2f}" if pd.notna(row.get('å¸‚ç›ˆç‡-åŠ¨æ€')) else "N/A")
            col_c.metric("èµ„äº§è´Ÿå€ºç‡ (%)", f"{row.get('èµ„äº§è´Ÿå€ºç‡(%)'):.2f}" if pd.notna(row.get('èµ„äº§è´Ÿå€ºç‡(%)')) else "N/A")
            
            st.text_area("è¡Œä¸šé¢˜æ", row.get('è¡Œä¸šé¢˜æ', 'N/A'), height=100, disabled=True)
